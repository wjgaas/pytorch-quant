{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "from torch.utils import data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = \"/home/jcwang/dataset/imagenet-data\"\n",
    "valdir = os.path.join(data_root, 'val')\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],       \n",
    "                                     std=[0.229, 0.224, 0.225])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-1.2445, -1.0219, -1.3987,  ..., -1.3987, -1.3302, -1.0390],\n",
      "          [-1.3473, -1.0562, -1.3473,  ..., -1.4158, -1.1075, -1.2274],\n",
      "          [-1.3473, -1.1075, -1.3130,  ..., -1.0390, -1.1760, -1.2445],\n",
      "          ...,\n",
      "          [-1.3987, -1.4158, -1.2788,  ..., -1.2959, -1.5185, -1.2274],\n",
      "          [-1.4329, -1.4843, -1.3815,  ..., -1.1932, -0.9705, -1.0562],\n",
      "          [-1.3473, -1.3815, -1.3302,  ..., -1.1760, -1.0219, -1.0219]],\n",
      "\n",
      "         [[-1.0903, -0.8452, -1.2129,  ..., -1.2654, -1.1779, -0.8627],\n",
      "          [-1.1429, -0.8803, -1.2304,  ..., -1.2829, -0.9678, -1.0903],\n",
      "          [-1.2654, -0.9503, -1.1954,  ..., -0.8978, -1.0203, -1.1429],\n",
      "          ...,\n",
      "          [-1.1954, -1.1954, -1.1429,  ..., -1.1779, -1.2479, -0.9853],\n",
      "          [-1.2129, -1.2129, -1.1604,  ..., -0.8978, -0.7927, -0.8627],\n",
      "          [-1.1604, -1.2304, -1.2829,  ..., -1.0378, -0.8627, -0.8803]],\n",
      "\n",
      "         [[-1.1944, -1.0724, -1.2990,  ..., -1.2293, -1.2293, -1.0027],\n",
      "          [-1.1944, -1.0201, -1.2641,  ..., -1.2467, -1.0724, -1.1073],\n",
      "          [-1.2816, -1.0027, -1.2119,  ..., -0.9330, -1.0898, -1.2467],\n",
      "          ...,\n",
      "          [-1.2467, -1.2990, -1.1596,  ..., -1.1421, -1.2293, -1.0550],\n",
      "          [-1.2467, -1.2816, -1.2467,  ..., -1.0376, -0.8458, -0.9678],\n",
      "          [-1.1944, -1.2641, -1.2641,  ..., -1.0550, -0.9853, -1.0027]]]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "workers    = 4\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.ImageFolder(valdir, transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "    ])),\n",
    "    batch_size =batch_size, \n",
    "    shuffle=True,\n",
    "    num_workers=workers, \n",
    "    pin_memory=True)\n",
    "val_size = len(val_loader)\n",
    "val_data = iter(val_loader)\n",
    "input,label = val_data.next()\n",
    "print(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-0.3883, -0.4226,  0.0056,  ...,  0.3823,  0.6392,  0.6221],\n",
      "          [-0.6965, -0.4397, -0.1486,  ...,  0.5878,  0.2111,  0.5536],\n",
      "          [-0.1486,  0.0741,  0.5878,  ...,  0.8961,  0.2282,  0.3138],\n",
      "          ...,\n",
      "          [-0.3883, -0.3541, -0.3541,  ..., -0.2856, -0.4226,  0.5536],\n",
      "          [-0.3198, -0.3369, -0.3541,  ...,  0.1083,  0.5364, -0.2684],\n",
      "          [-0.2513, -0.1657, -0.1486,  ..., -0.3369, -0.3198, -0.4739]],\n",
      "\n",
      "         [[-0.4076, -0.0399, -0.2675,  ...,  0.7479,  1.0280,  0.9930],\n",
      "          [-0.6352, -0.3550, -0.3550,  ...,  0.8880,  0.5378,  0.9230],\n",
      "          [-0.0924,  0.1176,  0.7129,  ...,  1.2906,  0.6604,  0.6954],\n",
      "          ...,\n",
      "          [-0.2500, -0.2325, -0.1975,  ..., -0.0924, -0.0399,  0.8529],\n",
      "          [-0.1099, -0.2325, -0.2325,  ...,  0.4153,  0.9580, -0.0224],\n",
      "          [ 0.3452,  0.3627,  0.3452,  ..., -0.0749, -0.1450, -0.3375]],\n",
      "\n",
      "         [[-0.5495,  0.0779, -0.5844,  ...,  0.7576,  0.7054,  1.0017],\n",
      "          [-0.7936, -0.4450, -0.6193,  ...,  1.0191,  0.6705,  0.8797],\n",
      "          [-0.4275, -0.1312,  0.2871,  ...,  1.3328,  0.7228,  0.6356],\n",
      "          ...,\n",
      "          [-0.2881, -0.2184, -0.1835,  ..., -0.2010, -0.2532,  1.0539],\n",
      "          [-0.0790, -0.2010, -0.2532,  ...,  0.4962,  1.1411,  0.1128],\n",
      "          [ 0.6705,  0.5834,  0.5659,  ..., -0.1138, -0.3230, -0.4798]]]])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception KeyError: KeyError(<weakref at 0x7fe33470f578; to 'tqdm' at 0x7fe333e5a090>,) in <bound method tqdm.__del__ of   0%|          | 0/1 [00:00<?, ?it/s]> ignored\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "n_sample = 1\n",
    "for idx, (data, target) in enumerate(tqdm.tqdm(val_loader, total=n_sample)):\n",
    "    print(data)\n",
    "    if idx >= n_sample - 1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
